{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models and Regression\n",
    "\n",
    "This worksheet will guide you to using regressionmodels. \n",
    "\n",
    "When you have classes of data (example cats and dogs...) you can classify these using classifiers. But what if you are trying to predict a value on a scale... For example you might want to predict how much a house costs based on its size. You cannot predict between a class...\n",
    "\n",
    "This tutorial will guide you through \n",
    "- downloading an open dataset\n",
    "- setting up various regression models\n",
    "- tuning hyperparameters to get the best outcome\n",
    "\n",
    "You will need to make sure you have installed the libraries of \n",
    "- kagglehub\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub\n",
    "!pip install -U scikit-learn\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #library for using large arrays in python\n",
    "import pandas as pd #library for playing with datasets\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "############# regression libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing a dataset\n",
    "Firstly take a look at Kaggle https://www.kaggle.com/ which is a great platform for people posting datasets. Have a look round, for now we wll be using a specific dataset but later on you should download datasets that interests you.\n",
    "\n",
    "Before we can install a dataset, we have to make an account, and use this account to sign in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to import the data we will use the open datasets library #we place the link copied from Kaggle into the parameter of the download function\n",
    "path = kagglehub.dataset_download(\"shree1992/housedata\")\n",
    "\n",
    "# Download latest version\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playing with our data\n",
    "\n",
    "Great now we have downloaded a dataset... but can we just plug this into a model? No! We have to look at our data, how it is formatted and how this looks. The data is downloaded in a folder in csv format. Try to think back to how we opened up a dataset from yesterdays lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASKS\n",
    "\n",
    "# open the dataset from the csv, you already have the path\n",
    "df = \n",
    "# visualise this dataset \n",
    "print(df.describe())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the data\n",
    "\n",
    "Before we get into anything fancy, lets train some regression models. Regression models take data in a linear format. By this I mean e need all the data to be represetned by a single vector. Lets say we only care about the bedrooms and bathrooms, and nothing else matters. We would then have [bedroom_no, bathroom_no] and a label [price of house]. The input data is the number of bedrooms andnumber of bathrooms... and our label data is the cost of that house. \n",
    "\n",
    "To extract this data is quite simple, we just grab the columns from the dataset that we want, convert to numpy and put them together (using the concatenate function).\n",
    "\n",
    "We have shown you how to do it for bedrooms and bathrooms. Play around and try add some other useful metrics into the input data. By the end we want to have a matrix n x m, where n is the number of samples, and m is the number of features we want to take input from. So for the above example m would be 2, bedrooms and bathrooms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can grab a column by using the column title \n",
    "bedrooms = df['bedrooms']\n",
    "bathrooms = df['bathrooms']\n",
    "#we can then convert the pandas object to numpy arrays so we can work with them for machine learning\n",
    "bedrooms = bedrooms.to_numpy()\n",
    "bathrooms = bathrooms.to_numpy()\n",
    "#when we add them together we dont just want to add them together and go from a dataset of n to 2n... so we create a new axis.\n",
    "print(\"Bathroom shape:\",bathrooms.shape)\n",
    "converted_bathroom=bathrooms.reshape((-1,1)) #-1 means ignore axis\n",
    "converted_bedroom=bedrooms.reshape((-1,1)) #-1 means ignore axis\n",
    "print(\"Bathrooms post processing:\",converted_bathroom.shape)\n",
    "#once they are in this shape we can add them together\n",
    "X_data=np.concatenate([converted_bathroom,converted_bedroom],axis=1) #axis is telling us where we want to combine them\n",
    "\n",
    "#or we could do all the above code in one line:\n",
    "X_data = np.concatenate([df['bedrooms'].to_numpy().reshape((-1,1)),df['bathrooms'].to_numpy().reshape((-1,1))],axis=1)\n",
    "\n",
    "\n",
    "#then to gather the labels we can use\n",
    "y_data=df['price']\n",
    "\n",
    "print(\"Expected data shape:\",X_data.shape)\n",
    "print(\"Expected label shape:\",y_data.shape)\n",
    "\n",
    "assert len(y_data)==len(X_data), \"incorrect shapes, x and y must be the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK\n",
    "\n",
    "#now add in a new column, or a few new columns from the dataset\n",
    "\n",
    "\n",
    "#this will make sure your code works, assuming you use the same variable names for your labels\n",
    "assert len(y_data)==len(X_data), \"incorrect shapes, x and y must be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know which data to use? Well to begin with you could go about plotting values and evaluating visually if there are links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['bedrooms'],df['price'])\n",
    "plt.grid(1)\n",
    "\n",
    "plt.xlabel(\"number of bedrooms\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.title(\"Price vs bedrooms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like number of bedrooms alone does not directly impact the price of a house. Have a look at some other variables and see if there are any that show relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our data to a train and test set\n",
    "\n",
    "It is important with models to train it on as much data as we can, but how does it perform on unseen data? We must reserve part of the dataset for testing. This tells us how well our model works outside what it has seen. If it does not perform well it suggests overfitting... or even a lack of relationship between your data and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "#another hyperparameter is the test_sixe, by default it is 20%, but does varying this size impact accuracy?\n",
    "\n",
    "#an idea for challenges once you have finished is to investigate this\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "#another hyperparameter is the random state. This does not impact the way the model performs significantly\n",
    "# but the way the data begins can change the accuracy. For fair experiments you should change the random state to random numbers\n",
    "# and then average the model performance over a number of trials\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=np.random.randint(0,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "This section is about importing various regression models and training. \n",
    "\n",
    "### Linear regression\n",
    "Linear regression is conseptually the most simple model. Lets take a look at implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Mean squared error (MSE):\", mean_squared_error(y_test, y_pred)) #this should be low\n",
    "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(1)\n",
    "plt.plot([df['price'].min(), df['price'].max()],\n",
    "         [df['price'].min(), df['price'].max()], 'k--')\n",
    "plt.title(\"Prediction vs real value\")\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at the graph and our MSE output, it seems the model has not performed too well. A good scatter of prediction vs real should be a diagonal line of points. Our data shows the predicted values are much lower than the real ones. It seems to predict low cost houses fairly well but not higher cost ones.\n",
    "\n",
    "Lets take a look at some other models\n",
    "\n",
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"Coefficients:\", ridge.coef_)\n",
    "print(\"Intercept:\", ridge.intercept_)\n",
    "print(\"Mean squared error (MSE):\", mean_squared_error(y_test, y_pred)) #this should be low\n",
    "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(1)\n",
    "plt.plot([df['price'].min(), df['price'].max()],\n",
    "         [df['price'].min(), df['price'].max()], 'k--')\n",
    "plt.title(\"Prediction vs real value\")\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not that much better than the linear. It seems our data might not be enough to capture a decent relationship. Try going back and adding more values into the X data. Does this improve the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK\n",
    "\n",
    "# train a model with more, lets include all values\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=np.random.randint(0,100))\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"Coefficients:\", ridge.coef_)\n",
    "print(\"Intercept:\", ridge.intercept_)\n",
    "print(\"Mean squared error (MSE):\", mean_squared_error(y_test, y_pred)) #this should be low\n",
    "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(1)\n",
    "plt.plot([df['price'].min(), df['price'].max()],\n",
    "         [df['price'].min(), df['price'].max()], 'k--')\n",
    "plt.title(\"Prediction vs real value\")\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has this improved it? If not perhaps there is not enough of a linear relationship between the data. Lets look at structures that are better with noisy data\n",
    "\n",
    "### Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"Mean squared error (MSE):\", mean_squared_error(y_test, y_pred)) #this should be low\n",
    "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(1)\n",
    "plt.plot([df['price'].min(), df['price'].max()],\n",
    "         [df['price'].min(), df['price'].max()], 'k--')\n",
    "plt.title(\"Prediction vs real value\")\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM\n",
    "Lets give GLM a go. For the task of house price predictions it might not be most optimal, but here is some demo code you can play around with\n",
    "\n",
    "Now when we say GLM we are more talking about logistic regression... which yes is classifiaction... Lets convert our dataset to a classification task. What if we wanted to instead of predict cost, predict whether we could afford the house?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_budget= #put your budget here, check the df['price'] to see what the values are, make sure your value is in this\n",
    "\n",
    "#now we convert the price to binary classes\n",
    "\n",
    "df['labels'] = df['values'] > our_budget\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have this the data in binary format we can classify it using logistic regression\n",
    "\n",
    "But you need to do this. We have our code from before on how to convert the pandas data to numpy input data and labels.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html go to the documentation</a> to find out how to implement a logistic regression function. \n",
    "\n",
    "You have all the basics in this sheet... start to use this to make your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "#train logistic regression\n",
    "\n",
    "#test how accurate it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions so far\n",
    "\n",
    "So far we have imported a dataset and played around with a few different hyperparameters. Go further to see how each hyperparemeter affects the accuracy. Try display this as a scientific experiment. \n",
    "\n",
    "Once you have optimised, play around with other datasets on kaggle. As long as the label is a continuous value rather than a classification task, the methods you learned here today are applicable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
